Image augmentation and data augmentation is one of the most widely used tools in deep learning to increase your dataset size and make your neural networks perform better. In this week, you'll learn how the use of the easy-to-use tools in TensorFlow to implement this. Yes. So like last week when we looked at cats versus dogs, we had 25,000 images. That was a nice big dataset, but we don't always have access to that. In fact, sometimes, 25,000 images isn't enough. Exactly. Some of the nice things with being able to do image augmentation is that we can then, I think you just use the term create new data, which is effectively what we're doing. So for example, if we have a cat and our cats in our training dataset are always upright and their ears are like this, we may not spot a cat that's lying down. But with augmentation, being able to rotate the image, or being able to skew the image, or maybe some other transforms would be able to effectively generate that data to train off. So you skew the image and just toss that into the training set. But there's an important trick to how you do this in TensorFlow as well to not take an image, warp it, skew it, and then blow up the memory requirements. So TensorFlow makes it really easy to do this. Yes. So you will learn a lot about the image generator and the image data generator, where the idea is that you're not going to edit the images directly on the drive. As they get float off the directory, then the augmentation will take place in memory as they're being loaded into the neural network for training. So if you're dealing with a dataset and you want to experiment with different augmentations, you're not overriding the data. So [inaudible] to generate a library lets you load it into memory and just in memory, process the images and then stream that to the training set to the neural network we'll ultimately learn on. This is one of the most important tricks that the deep learning [inaudible] realizes, really the preferred way these days to do image augmentation. Yeah and I think it's, for the main reason that it's not impacting your data, right, you're not overriding your data because you may need to experiment with that data again and those kind of things. It's also nice and fast. It doesn't blow up your memory requirements. You can take one image and create a lot of other images from it, but you don't want to save all those other images onto this. Remember, we had a conversation recently about the lack of, there's a lot of literature on this topic so there's opportunity to learn. Yeah. One of these thinkings about data augmentation and image augmentation is so many people do it, it's such an important part of how we train neural networks. At least today, the academic literature on it is thinner relative to what one might guess, given this importance, but this is definitely one of the techniques you should learn. So please dive into this week's materials to learn about image augmentation and data augmentation.


You'll be looking a lot at Image Augmentation this week.

Image Augmentation is a very simple, but very powerful tool to help you avoid overfitting your data. The concept is very simple though: If you have limited data, then the chances of you having data to match potential future predictions is also limited, and logically, the less data you have, the less chance you have of getting accurate predictions for data that your model hasn't yet seen. To put it simply, if you are training a model to spot cats, and your model has never seen what a cat looks like when lying down, it might not recognize that in future.

Augmentation simply amends your images on-the-fly while training using transforms like rotation. So, it could 'simulate' an image of a cat lying down by rotating a 'standing' cat by 90 degrees. As such you get a cheap way of extending your dataset beyond what you have already.

To learn more about Augmentation, and the available transforms, check out https://github.com/keras-team/keras-preprocessing -- and note that it's referred to as preprocessing for a very powerful reason: that it doesn't require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you to experiment without impacting your dataset.

