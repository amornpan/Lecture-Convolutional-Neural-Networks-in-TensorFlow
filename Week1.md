In the first course, you learned how to use TensorFlow to implement a basic neural network, going up all the way to basic Convolutional Neural Network. In this second course, you go much further. In the first week, you take the ideas you've learned, and apply them to a much bigger dataset of cats versus dogs on Kaggle. Yes so we take the full Kaggle dataset of 25,000 cats versus dogs images. In the last module, we looked at horses and humans, which was about 1,000 images. So we want to take a look at what it's like to train a much larger dataset, and that was like a data science challenge, not that long ago. Now, we're going to be learning that here, which I think is really cool In fact, we have substantially similar ideas as their previous goals, and apply it to much bigger datasets, and hopefully get great results. Yeah, we're hoping for good results. Let's see what the students get as they do some of the assignments with it as well. One of the things that working with a larger dataset, then helps with is over-fitting. So with a smaller dataset, you are at great risk of overfitting; with a larger dataset, then you have less risk of over-fitting, but overfitting can still happen. Pretty cool. Then in week 2, you'll learn another method for dealing with overfitting, which is that TensorFlow provides very easy to use tools for data augmentation, where you can, for example, take a picture of a cat, and if you take the mirror image of the picture of a cat, it still looks like a cat. So why not do that, and throw that into the training set. Exactly. Or for example, you might only have upright pictures of cats, but if the cat's lying down, or it's on its side, then one of the things you can do is rotate the image. So It's like part of the image augmentation, is rotation, skewing, flipping, moving it around the frame, those kind of things. One of the things I find really neat about it, is particularly if you're using a large public dataset, is then you flow all the images off directly, and the augmentation happens as it's flowing. So you're not editing the images themselves directly. You're not changing the dataset. It all just happens in memory. This is all done as part of TensorFlow's Image Generation [inaudible]? Exactly. That they'll learned about it in the second week. Yeah. So then too one of the other strategy, of course for avoiding overfitting, is to use existing models, and to have transfer learning. Yeah. So I don't think anyone has as much data as they wish, for the problems we really care about. So Transfer Learning, lets you download the neural network, that maybe someone else has trained on a million images, or even more than a million images. So take an inception network, that someone else has trained, download those parameters, and use that to bootstrap your own learning process, maybe with a smaller dataset. Exactly. That has been able to spot features that you may not have been able to spot in your dataset, so why not be able to take advantage of that and speed-up training yours. I particularly find that one interesting as you move forward. That to be able to build off of other people's work, the open nature of the AI community, I find is really exciting and that allows you to really take advantage of that and be a part of the community. Standing on the shoulders of giants. I use transfer learning all the time, so TensorFlow lets you do that easily [inaudible] open source. Then finally in the fourth week, Multicast learning. Rather than doing two classes, like horses verses humans, or cats verses dogs, what if you have more than two classes, like class five rock, paper, scissors, that would be three classes, or inception would be 1,000 classes. So that the techniques of moving from two to more than two, be it three or be it a 1,000, are very very similar. So we're going to look at those techniques and we'll look at the code for that. So and we have a rock, paper, scissors example, that you're going to be able to build off of. So in this second course, you take what you learned in the first course, but go much deeper. One last fun thing, Lawrence had seen this coffee mug into AI for everyone in the course, and he asked me to bring it. I love that course, so thank you so much. It's a great course, because it has got everything for people who are beginning; even people who are non-technical, all the way up to experts. So thank you for the mug, but is it okay if I say I spot a sports car in the course as well, would you bring that? I don't have one of those to bring to you. So I'm really excited about this course. Please go ahead and dive into the first of the materials for week 1.
